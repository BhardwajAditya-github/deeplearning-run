{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets peft accelerate bitsandbytes torch --quiet\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import os"
      ],
      "metadata": {
        "id": "DGJPxAUSRnMn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# ====== Step 1: Load Model and Tokenizer ======\n",
        "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "device = \"cpu\"  # Ensure we're using CPU'\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=6\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "yzInxrJSRoDi",
        "outputId": "7874f20f-77a8-40c3-e6fc-f3186b14cb97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Step 2: Sample Data ======\n",
        "sample_conversations = [\n",
        "    {\n",
        "        \"conversation\": (\n",
        "            \"Caller: I need you to send money immediately, I am in big trouble!\\n\"\n",
        "            \"Receiver: What happened? Why do you need money so urgently?\\n\"\n",
        "            \"Caller: I got into a legal issue, and I need bail money right now!\"\n",
        "        ),\n",
        "        \"label\": \"Legal/Authority Urgency\"\n",
        "    }\n",
        "]\n",
        "\n",
        "labels = [\"Emotional Urgency\", \"Financial Urgency\", \"Legal/Authority Urgency\",\n",
        "          \"No Urgency\", \"Social/Peer Pressure Urgency\", \"Romantic Urgency\"]\n",
        "label_to_id = {label: i for i, label in enumerate(labels)}"
      ],
      "metadata": {
        "id": "co3-7MrbRvfr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Step 3: Tokenization and Prediction ======\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "for conv in sample_conversations:\n",
        "    inputs = tokenizer(\n",
        "        conv[\"conversation\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    print(f\"Conversation: {conv['conversation'][:100]}...\")\n",
        "    print(f\"Actual Label: {conv['label']}\")\n",
        "    print(f\"Predicted Label: {labels[prediction]}\")\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "4oMdQ4m2RzSL",
        "outputId": "cb40ea74-8c80-4a96-f70c-7231dd6e4513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation: Caller: I need you to send money immediately, I am in big trouble!\n",
            "Receiver: What happened? Why do y...\n",
            "Actual Label: Legal/Authority Urgency\n",
            "Predicted Label: Social/Peer Pressure Urgency\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Step 4: Load and Prepare Dataset ======\n",
        "df = pd.read_csv(\"./urgency_data.csv\")\n",
        "print(df.head())\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"label_encoded\"] = label_encoder.fit_transform(df[\"label\"])\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label Mapping:\", label_mapping)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"conversation\"].tolist(),\n",
        "    df[\"label_encoded\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training Samples: {len(train_texts)}, Validation Samples: {len(val_texts)}\")"
      ],
      "metadata": {
        "id": "eSHvZWbQR4zO",
        "outputId": "41da5b9e-d824-40f4-e6fc-d146762e86ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        conversation              label\n",
            "0  Caller: Hello, I'm so sorry to call you with t...  Emotional Urgency\n",
            "1  Caller: Hi, I'm so sorry to inform you, but yo...  Emotional Urgency\n",
            "2  Caller: This is terrible news, but I'm afraid ...  Emotional Urgency\n",
            "3  Caller: I'm so sorry to call you with this, bu...  Emotional Urgency\n",
            "4  Caller: Hello, I'm calling about your cousin. ...  Emotional Urgency\n",
            "Label Mapping: {'Emotional Urgency': 0, 'Financial Urgency': 1, 'Legal/Authority Urgency': 2, 'No Urgency': 3, 'Romantic Urgency': 4, 'Social/Peer Pressure Urgency': 5}\n",
            "Training Samples: 450, Validation Samples: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Step 5: Dataset and Dataloaders ======\n",
        "class UrgencyDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "train_dataset = UrgencyDataset(train_encodings, train_labels)\n",
        "val_dataset = UrgencyDataset(val_encodings, val_labels)\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Dataloaders Ready! Batch Size:\", BATCH_SIZE)"
      ],
      "metadata": {
        "id": "j-OdIVerR8xv",
        "outputId": "db1de062-f741-4b46-a7c7-bd077355458f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders Ready! Batch Size: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Step 6: Training Arguments ======\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=0.01,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"adamw_torch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    run_name=\"deepseek-cpu-finetune\",\n",
        "    fp16=False\n",
        ")\n",
        "\n",
        "print(\"Training arguments set for CPU!\")"
      ],
      "metadata": {
        "id": "a-JnOKh_R_lz",
        "outputId": "a4db445a-cf21-4539-b804-9786190be19f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments set for CPU!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Step 7: Train Model ======\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./deepseek-finetuned\")\n",
        "tokenizer.save_pretrained(\"./deepseek-finetuned\")\n",
        "\n",
        "print(\"Training complete! Model saved successfully.\")"
      ],
      "metadata": {
        "id": "4i4L2IOESEmV",
        "outputId": "705c2cd8-3722-4138-d37b-8b2dada6aec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d688d8e96d59>:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}